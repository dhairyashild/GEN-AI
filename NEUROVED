algorith  = way to learn by m/c

algo+ data = ai/ml model

ML 1950== ML
PREDICTION  === 500 FEET SPACE WHAT PRICE BY LINE ON CHART
CLASSIFICATION = HE IS PASS OR NOT   SAME E LINE HIKADCHE PASS LINECHYA AND TIKADCHE FAIL

2000 = DEEP LEARN
FIND DOG , CAT
- BUT NO MEMORY

2010= RNN , LSTM
HAVE SMALL MEMORY ONLY

2017 -- PAPER BY GOOGLE "ATTENTION ALL yoU NEED"
THEY GAVE TRANSFORER MODEL = HAVE LARGE MEMORY ALSO WHICH NOT THERE IN RNN + LSTM
-TRANSFORMER == 
1)ENCODING  - CONVERT WORD INTO NUMBER --GOOGLE WORK ON IT AND DEVELOP= BERT
2)DECODING = PREDICT NEW WORDS - OPENAI OF ALTMAN GAVE = 2017=GPT-117 MILLION PARA  

A Transformer = 
DEF ===is a neural network architecture that understands context by looking at an entire sentence at once, rather than one word at a time.
    -uses Self-Attention to look all words in sentence simultaneously & decide which  are most relevant to eachother

1. The Core Innovation: "Self-Attention"
-In older models (RNNs/LSTMs), the AI read text like a humanâ€”one word at a time. 
It often "forgot" the beginning of a long sentence by the time it reached the end.
-The Transformer uses Self-Attention to look at every word in a sentence simultaneously and 
decide which ones are most relevant to each other.
-Example: In the sentence "The animal didn't cross the street because it was too tired,
" Self-Attention helps the model mathematically link the word "it" to "animal" rather than "street."

2. Key Components
The Encoder: Reads the input text and converts it into a rich numerical representation (understanding the context).
Positional Encoding: Since Transformers process words all at once, they don't naturally know the order of words. This adds a "time stamp" to each word so the model knows its position.
The Decoder: Takes that representation and predicts the next token in the sequence (generating the response).
Multi-Head Attention: This allows the model to focus on different parts of the sentence for different reasons (e.g., one "head" focuses on grammar, another on the subject-verb relationship).
-----------------------------------
2022= ANTHROPIC --- CLAODE MODEL 4.1 CURRENT
      GOOGLE   ---- BARD , GEMINI MODEL
META --- LLAMA
MISTRAL -- MISTRAL
2025- DEEPSEEK

-SO APROX 1 BILLION PARA NEED 2.5 GB RAM GPU  ,  1BN PARA = 2.5 RAM
SO 600 GB MODEL NEED 1500 GB RAM IMPOSIBLE FOR NORMAL PEOPLE

SO API CAME = API OF LLM HOSTED BY LARGE COMPANY WHO HAVE INFRA LIKE=
AWS -BEDROCK
AZUREAI
SAMNOVA
GROQ




-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  UV + GIT BASH (WINDOWS) - PERFECT CHEAT SHEET ðŸš€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1. INSTALL & PATH (Git Bash)
curl -LsSf https://astral.sh/uv/install.sh | sh
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.bashrc && source ~/.bashrc
uv --version

# 2. NEW PROJECT
mkdir my-project && cd $_
uv init        # Creates pyproject.toml
uv venv        # Creates .venv folder

# 3. ACTIVATE (Git Bash ONLY)
source .venv/Scripts/activate   # Forward slashes, Scripts NOT bin!

# 4. INSTALL PACKAGES (auto-saves to pyproject.toml)
uv add openai langchain torch transformers jupyter pandas numpy

# 5. JUPYTER SETUP
uv run ipython kernel install --user --name="myenv"
uv run jupyter notebook

# 6. RUN (without activation - pro move)
uv run python script.py

# 7. COPY PROJECT TO NEW FOLDER ðŸ”‘
#    Step 1: Copy ONLY these two files:
#    - pyproject.toml
#    - uv.lock
#    Step 2: In new folder:
uv sync        # Recreates EXACT environment (versions locked!)
source .venv/Scripts/activate

# 8. USEFUL COMMANDS
uv add package          # Install + add to pyproject.toml
uv remove package       # Uninstall + remove from toml
uv lock                 # Update uv.lock only
uv tree                 # Show dependency tree
uv pip list             # List installed packages

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GOLDEN RULES:
# â€¢ Windows/Git Bash = source .venv/Scripts/activate
# â€¢ Mac/Linux = source .venv/bin/activate
# â€¢ uv sync = "Clone this environment" (needs lock + toml)
# â€¢ uv run = "Run without thinking about activation"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##################################################################
# --- UV ENVIRONMENT: COPY TO ANOTHER FOLDER ---

# 1. PREP: Copy 'pyproject.toml' (and 'uv.lock') to the new project folder #
# 2. SYNC: Run 'uv sync' (This builds a fresh .venv based on the copied files) #
# 3. ACTIVATE: 
#    - Windows: source .venv/Scripts/activate #
#    - Mac/Linux: source .venv/bin/activate #

# --- QUICK UV CHEAT SHEET ---
# uv init          -> Start new project #
# uv add <package> -> Install & save to toml #
# uv run <file.py> -> Run script without activating venv #
# uv lock          -> Refresh the lockfile only #
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# à¤¸à¤¾à¤°à¤¾à¤‚à¤¶ (SUMMARY) NO NEED TO READ BELOW ALL JUST 1 LINE OK      -- UV AND LIB AND FUNCTION MEANTING  ====from [LIBRARY_IN_NEUROVED] import [FUNCTION] 
--------------------------------------------------------------------------------------------------------------
# PROJECT CONTEXT: You are working in ~/OneDrive/Desktop/NEUROVED-CODES

# 1. WHERE PYTHON LOOKS
# Why: 'uv' stores your AI tools in hidden folder inside current project.  === Path: ~/OneDrive/Desktop/NEUROVED-CODES/.venv/lib/site-packages/

# 3. HOW IT WORKS IN YOUR TERMINAL
# Why: 'uv run' tells Python to look ONLY in your 'NEUROVED-CODES' environment.
uv run python main.py # Executes your file using the tools found in your local .venv.

# from [LIBRARY] import [FUNCTION]  ==== Access the (Folder) to use the (Reusable Code).

from openai import OpenAI # Go to 'NEUROVED-CODES/.venv/...' and grab the 'OpenAI' tool.
from transformers import pipeline # Go to 'NEUROVED-CODES/.venv/...' and grab the 'pipeline' tool.

# from [LIBRARY_IN_NEUROVED] import [FUNCTION] # Keeps your 'main.py' fast and clean.
# 1. LIBRARY   ==== folder containing many files of pre-written code for a specific topic.
# 2. FUNCTION (The Reusable Tool) ===specific block of code inside a library designed to perform one task.

---------------------------------------------------------------------------------------------------------------------------------------------
Context Window: Max tokens (input+output) LLM processes at one responce.
Temperature == its fature that controls randomness and creativity of  LLM's output.: 0.0 = deterministic, 2.0 = highly creative. [Your Llama model uses 0.7]
Top-P (Nucleus Sampling):- selects tokens from the smallest group of tokens whose total probability reaches $P$ (usually 0.9 or 0.95).
                         - Filters out "long-tail" low-probability junk while keeping diverse, high-quality options.
                         - Impact: High $P$ (0.9) adds creative variety; low $P$ (0.1) forces safe, logical responses.
Top-K: - Picks tokens only from top $K$ most likely words (e.g., $K=40$).
       - Fixed Pool: Unlike Top-P, it uses a static number (e.g., $K=40$) regardless of how high or low the probabilities are.
       - Impact: Lowering $K$ makes the AI more predictable; increasing $K$ adds randomness and variety.

Max Tokens: MAX number of tokens the model can generate in a single response.[Your model: max_new_tokens=512]

gpt 3.5 = 4096 tokens 
gpt5 token window = 128k tokens 
llama2 =4096 tokens 
gemini =1 million token [have largst token no.]

--------------------------------------------------------------------------------------------------------------
1 token = 4 char [char includes words , symbol@ and space]

llm creates one word which has highest probabilty at a time in output , 
llm=neural network trained on massive datasets to predict the next most likely token in a sequence
    dictionary of vocabs consist of general info 

temprature = Its fature that controls randomness and creativity of  LLM's output
- low temp -> low creativity , high temp -> high creativity 
- same answ for same question again  --> t low -> 0-0.2 -> for factual answer --> we kp low like medical 
- high temp --> movie ,poem , essay , 
- mat -- 0.9 [probability]
  gate -- 0.02
  sofa -- 0.03
temp=2
new_probability_of_mat_if i change temperature to 2 = 0.9/[0.9/2 + 0.02/2  + 0.03/2]
new_probability =[logits/temperature]*softmax
Setting $T (Low) makes the highest-probability tokens even more likely to be chosen.
Setting $T > 1$ (High) makes the lower-probability tokens more likely to be chosen.

top-k   -> Picks tokens only from the top $K$ most likely words (e.g., $K=40$)., ignores all others.
           - for below example if we give k=4 thn it will select first 4 tokens only and neglect next


the       0.3
capital   0.2
of        0.1
india     0.1   

top-p = 0.7  top-p is 0.7 given  then above 4 words will be given because they total 0.7 prob and remove low prob below 
               words 
is        0.1   dis
new       0.1   dis
delhi     0.05  discards    
kolkatta  0.05  dis
raipur    0.05  dis
bhopal    0.05  dis
total     1     

SUMMARY 
temp                             top-p                    top-k
controls randomness            p=0.9-0.95                k=50 it will select top 50 words geenerated from llm vocab 
low temp -> low creativity ,    effect creativity         
high temp -> high creativity 
T = 0-0.2 FActual
    0.4-1 balanced creativity
    1<    high creat

-  we mostly used 1)temp  , 2) maxtokens to control output

langchain= framwork to create llm based apps
langchain and llamaindex same but in llama index small pay required 
-  langchain llm based app
-  langgrapph agentic ai
-  human mssage  =anything we ask to llm
-  system messag =role we assign to llm 
-  ai msg        =output msg 

âŒ WITHOUT LangChain (have to write custom code for every model )
from huggingface_hub import InferenceClient
client = InferenceClient(token="hf_xxx")
response = client.text_generation("Hello!", model="meta-llama/Llama-3.1-8B-Instruct")
print(response)

âœ… WITH LangChain (10 lines - Your chatbot)
from langchain_huggingface import HuggingFaceEndpoint
llm = HuggingFaceEndpoint(repo_id="meta-llama/Llama-3.1-8B-Instruct")
model = ChatHuggingFace(llm=llm)
response = model.invoke("Hello!")
print(response.content)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------- lecture 6 ------------------------------------------------------
- in messages first it runs system message then work on human message 
-  SystemMessage =role w assign to llm -> if we give role as coding expert and ask question on heart attack [medical question] then LLM will still answer the medical question but frame it through a coding lens
-  HumanMessage  =anything we ask to llm
-  AIMessage : o/p
Primary: Stores AI model responses in conversation history
Secondary: Can be used in few-shot examples (with example=True)
("system","act as helpful assistant and translate {ip_language} into {op_language}" ),      ## REMEMBER   (   "system" , "           "  )   OR  SystemMessagePromptTemplate.from_template("act as helpful assistant...")  ==BOTH WORKS SAME
("user","{text}")                                                                           ## REMEMBER   (   "user"   , "           "  )   OR  HumanMessagePromptTemplate.from_template("act as helpful assistant...")   ==BOTH WORKS SAME
])

# CORRECT CODE: Chain setup using LangChain and ChatOpenAI

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1. TEMPLATE: Define the logic and variables.
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that translates {input_language} to {output_language}"),
    ("human", "{input}")
])

# 2. CHAIN: Combine the prompt and the model using the pipe (|) operator.
model = ChatOpenAI(model="gpt-4o")
chain = prompt | model

# 3. INVOKE: Send data and get the result.
output = chain.invoke({                   # chain.invoke la format lagat nahi -> ready_prompt.format la lagate 
    "input_language": "English", 
    "output_language": "Odia", 
    "input": "I love programming"
})
print(output.content) # Result: à¬®à­à¬ à¬ªà­à¬°à­‹à¬—à­à¬°à¬¾à¬®à¬¿à¬‚à¬•à­ à¬­à¬² à¬ªà¬¾à¬ |

transaction list example :

first give ur basic promt and ask gpt to refine ur prompt because o/p depends on it.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------- lecture 7 ------------------------------------------------------
chatPromptTemplate consist of list of tuples
chatPromptTemplate=[(),()]
output.response_metadata = no.of tokens + other info

- SystemMessage =role w assign to llm -> if we give role as coding expert and ask question on heart attack [medical question] then LLM will still answer the medical question but frame it through a coding lens
- chain.invoke("Tata")
- chain.invoke("stock":"Tata")
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------- lecture 8 ------------------------------------------------------
-  if u dont give human message and system mssage and ai message then use
model.invoke("what is python")

-   if u dont want ai message then use
msg=[("humanmessage","what is python")
("systemmessage","act as ai engineer")]
model.invoke(msg)
-  when you want to input variable
message = ['system', You are experienced (role). Answer all the query related to that only else say sorry,
("human', 'this is the (query)')]

types
1- 0 shot
   2.1 Zero-Shot Prompting = 0 EXAMPLE
   What it is: Asking the Al to perform a task without providing any examples.
   When to use: For straightforward tasks where the Al already understands what you want.   
   sys_msg = "you are experince reviwe critic, based on the review tell weather the product belong to which categories 1.good, 2.bad and 3. ok" 
   human_msg="These is the review: (review)"
   msg = [('system', sys_msg), ('human', human_msg)]
   template =ChatPromptTemplate(msg)

2- one shot prompting = 1 EXAMPLE         - 2 human + 1 ai message + 1 system message fixed
   What it is: Providing exactly 1 EXAMPLE to show the Al what you want.
   When to use: When you need to demonstrate a specific format or style.
   Structure: It requires 2 Human Messages and 1 AI Message in the prompt history:
       - Human Message 1: The example input (e.g., "Apple -> Red").
       - AI Message 1: The example output (e.g., "Fruit").
       - Human Message 2: The actual request (e.g., "Carrot -> ").
   #One-shot example: Product description generation

   one_shot_template Chat PromptTemplate([
   "system", "You generate product name and parameters for e-commerce review message"),
   'human", "I've been using the Samsung Galaxy S24 Ultra (12GB RAM 256GB Storage) for over a month now, and it has exceeded my expectations in almost every area.").
   ai", "product name: Samsung Galaxy 524 Ultra, parametrs:12GB RAM 256GB Storage").
   ['human", "this is the e-commerce review message: (rev_msg}")])
   chain one shot template | llm
-  *** in this we use 2 human message - first human mssage is for giving 1 example and other  for human input ***

2- few shot
   giving more than 1 ex is better but it increases = 1] token     2] response time from llm


3- 2 shot


4- chain of thought = Prompting technique that forces the AI to show its step-by-step reasoning before giving the final answer.  
   What it is: Asking the Al to think step-by-step and show its reasoning.
   When to use: For complex problems that require logical reasoning or multi-step solutions.
   Chain-of-thought example: Math word problems
   cot_template= ChatPromptTemplate.from messages(
"system", "You solve math problems step by step. Always show your reasoning clearly.").
"human", """Let's work through this step by step.
Problem: [problem] Please think through this step by step:
1. What information do we have?
2. What do we need to find?
3. What steps do we need to take?
4. Show the calculations
5. State the final answer
   Test with a word problem
math problem = "A store sells books for $15 each. If they offer a 20% discount for buying 5 or more books, how much would it cost to buy 8 books?

5- tree of thought  = Prompting technique that forces the AI explores multiple branches of thought simultaneously, rather than just one linear path.

6- 
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------- lecture 9 ------------------------------------------------------
Stream lit
- To create web app
- to show POc (proof of Concept)
- automatically create html in background 
- 
uv add streamlit
- setting run on save -> if u change in main.py then automaticaaly changes will reflect in ui
- st.set_page_config("first_app")  
- st.text_area("This app is about chat and asking question about any topic")

img = st.file_uploader("Upload any Image")
if img:
st.image(ing)    #st.audio
or st.video

file_name=st.text_input("Enter the url
if file name:
click =st.button("Press Enter")
if click:
st.video(file_name)

st.sidebar.title("About")
options =st.sidebar.radio("select one of the options:", ["audio", "video", "image"])
options =st.radio("select one of the options:", ["audio", "video", "image"])

option2 =st.selectbox("select one of the options:", ["audio", "video", "image","pdf"])





-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------- lecture 10 ------------------------------------------------------
#  both works same just writing area size chages  (area > input)
st.text_input(       )            ------it gives small area to write
st.text_area(       )           ------it gives large area to write

Multimodal == Model that processes MULTIPLE data types simultaneously. so text + all other (image + audio + video ...).
              But mahjority llm support text based i/p only so llm which suppot img/audio ...called as multimodel

link for multimodal code---            
https://docs.langchain.com/oss/python/langchain/messages#multimod

On this page(right side sidebar)----->multimodal  --- code block has below 4 types in it
Image input
PDF document input
Audio input
Video input


# 1. THE PART YOU WRITE (Custom Helper)
import base64, mimetypes #

def encode_image(path): #
    mime, _ = mimetypes.guess_type(path) #
    with open(path, "rb") as f: #
        b64_str = base64.b64encode(f.read()).decode('utf-8') #
    return b64_str, mime #

# 2. THE PART YOU GET FROM THE SITE (LangChain Multimodal Schema)
# Find in: "Multimodal" -> "From base64 data"
img_b64, img_mime = encode_image("NEUROVED.jpg") #

message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Describe the content of this image."},
        {
            "type": "image",
            "base64": img_b64,
            "mime_type": img_mime,
        },
    ]
}

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------- lecture 11------------------------------------------------------
# 1. CHAT INPUT WITH FILES
st.chat_input("Say something", accept_file=True)  # Single file
st.chat_input("Upload", accept_file="multiple")   # Multiple files

# 2. FILE UPLOADER (classic)
st.file_uploader("Upload", type=['png','pdf','csv'])  # Single
st.file_uploader("Upload", accept_multiple_files=True)  # Multiple

# 1. CHAT INPUT - DIRECTORY UPLOAD
st.chat_input("Upload folder", accept_file="directory")  # Upload entire folder!
# 2. MULTIPLE FILES + DIRECTORY
st.chat_input("Upload", accept_file=["multiple", "directory"])  # Both allowed


-IF  we change anything in streamlit , whole program rus again

-SIMPLE EXAMPLE ===

st.title("Chat App")
prompt = st.chat_input("Say Something")

if prompt:
    with st.chat_message("user", avatar="ðŸ§‘"):
        st.write(prompt)
    
    with st.chat_message("ai", avatar="ðŸ¤–"):
        output = llm.invoke(prompt)
        st.markdown(output.content)




################################  SEE SESSION STATE AFTER 40 MINUTES AGAIN


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------             VIDEO 12  = RAG INTRO               ----------------------------------------------------------------------------------------------------------------------------------------------
# LLM LIMITATIONS â†’ RAG SOLVES âœ…
# 1. HIGH GPU TRAINING COST â†’ NO TRAINING NEEDED! Just retrieve
# 2. STATIC KNOWLEDGE â†’ FETCH LIVE/CURRENT DATA from vector DB
# 3. SMALL CONTEXT WINDOW â†’ ONLY RELEVANT chunks injected
# 4. HIGH COST (prompt stuffing) â†’ CHEAPER, fewer tokens
# 5. HALLUCINATIONS â†’ GROUNDED in retrieved facts
# 6. PRIVATE DATA â†’ SECURE, don't fine-tune on sensitive data

-- RAG DIAGRAM EXPLAINED
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------                  video 13          -----------------------------------------------------------------------------
RAG STEPS===

1- DOCUMENT LOADER 
WRAPPPER IS LIBRARY GIVEN BY LANGCHAIN SO COMMOM COE TO LOAD ALL DOCS WITH LANG
DOCUMENT( "PAGE INFO",
           "METADATA",
           "ID (OPTIONAL)" )


# UV ADD 
uv add langchain_community
Added to TOML: Yes. It automatically updates your --->     pyproject.toml   +   uv.lock.

If you are in the folder but don't see (.venv) at the start of your line, it means the environment is sleeping. 
You are in the right place, but your terminal is still using the "global" Python.

Here is the shortest way to wake it up in MINGW64 / Git Bash:
source .venv/Scripts/activate              /   . .venv/Scripts/activate     --- BOTH SAME CHECK

If you still don't see the (.venv) text (sometimes the display is buggy), run this:
which python
Success: It shows a path inside your NEUROVED-CODES folder (UR CRRENT FOLDER) .
Failure: It shows a path like /c/Users/Admin/AppData/Local/Programs/Python....
######################################################################
# 1= LOAD PDF          ===  link

https://docs.langchain.com/oss/python/integrations/document_loaders/pypdfloader
1- installation command to get lib
2- initialization code to get below code

from langchain_community.document_loaders import PyPDFLoader
# Your existing code below
file_path = "C:/Users/dhair/OneDrive/Desktop/NEUROVED-CODES/shubhamlondhe_ansible_notes.pdf"    # my pc path
loader = PyPDFLoader(file_path)
# 2. Extract Data
docs = loader.load() # Execute loading into memory
#   BELOW MY COMMANDS TO GET INF
len(docs)
docs[0]
# 3. Access Content
print(docs[0].page_content) # Print first page text
print(docs[0].metadata) # Print file source and page number
print(docs)
######################################################################
# LOAD WEB SCRAPING --- 1ST 3 LINES FOR PYDANTIC WARNING IF NOT IVE ITS OK.
import os, warnings                                                           ### MY CODE = 3#
# MUST BE AT TOP: Hides the Python 3.14 / Pydantic V1 warning #
warnings.filterwarnings("ignore")                                                      ###                              

# MUST BE BEFORE LOADER: Identifies you to the website to prevent being blocked #  
os.environ["USER_AGENT"] = "Mozilla/5.0"         ###All modern browser (including Chrome) uses "Mozilla" in its ID string to ensure compatibility with old websites ,otherwise block them.

# Import only after the warning-ignore line above is set
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://python.langchain.com/docs/introduction/")

docs = loader.load()                                       ###  

# Use print() to see results in a .py script              ###
print(f"Total Pages: {len(docs)}")                          ###  
print(f"Snippet: {docs[0].page_content[:150]}")                         ###  

####################################   INTRVIEW POOINTS ON THIS  ##################################
dada loading --- include cleaning (crawlAI library need for cleaning)
 IN DOCUMENT LOADER----UNSTRUCUTRED ---FOR GETTING DFWITH IMAGE/TABLES
AUDIO LOADING NEED TO CONVERT SPEECH TO TEXT THEN ONLY LLM CAN WORK ON IT
LLM WE GET HUNGGINGFACE THEY NOT STOR E MY DAT BUT CHATGPT STORE MY DATA
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 PyMuPDFLoader--- SO USE THIS INSTEAD OF PYPDFLADER

# --- INDUSTRY COMPARISON ---
--------------------------------------------------------------------------
# Feature    | PyMuPDFLoader (Pro)          | PyPDFLoader (Basic)
--------------------------------------------------------------------------
# Speed      | 10x Faster (C++ engine)      | Slower (Pure Python)
# Layout     | Best (Tables/Columns)        | Poor (Scrambles columns)
# Metadata   | Rich (Author, Title, Tags)   | Basic (Page # only)
# Library    | pip install pymupdf          | pip install pypdf
--------------------------------------------------------------------------

# uv add  PyMuPDFLoader
from langchain_community.document_loaders import PyMuPDFLoader
# 1. Setup (Requires: uv add pymupdf)
loader = PyMuPDFLoader("ansible_notes.pdf") # Identify PDF #
# 2. Load
docs = loader.load() # Fast extraction of text and metadata #
# 3. Use
print(docs[0].page_content) # Access page 1 text #
###################################################
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# ============================================
# 9. AGENTIC AI (The "Autonomous" Part)
# ============================================
# Agent Loop: Thought -> Action -> Observation -> Final Answer.
# Tool Use: LLM doesn't "know" the weather; it "calls" a function to get it.
# ReAct: A framework where the model explains its reasoning before acting.
# Pydantic: Forces LLM to output valid JSON for software to read.

# 10. DEPLOYMENT & PRODUCTION
# -----------------------------
# Quantization: Shrinking a model (e.g., 16-bit to 4-bit) to run on low RAM.
# LoRA: Efficient fine-tuning by only changing a tiny part of the model.
# Groq/vLLM: Tools used to make LLM responses extremely fast (High Inference).
# Hallucination Fix: RAG + Temp 0.0 + Strict System Prompt.

# 11. VERSION CONTROL PRO-TIPS
# -----------------------------
# git status: Check what changed.
# git pull: Get latest team code.
# git push: Submit your AI features.
# .gitignore: Essential to hide your .env (API Keys) and .venv folder.

# ============================================
# QUICK DEFINITION RECALL
# ============================================
# 1. Embedding: Text turned into a math vector.
# 2. Vector DB: Where embeddings are stored for fast search.
# 3. Chain: A sequence of links (Prompt -> Model -> Parser).
# 4. Multimodal: Model that "sees" images and "hears" audio.
# 5. Token Limit: Why long chats eventually "forget" the beginning.
# ============================================

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


